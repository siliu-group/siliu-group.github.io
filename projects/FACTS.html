<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <meta name="description" content="BeautyGAN" />
    <meta name="keywords" content="GANs" />
    <meta name="author" content="" />
    <link href="../css/PP-GANs.css" rel="stylesheet" type="text/css" />
    <title>FACTS</title>
</head>

<body>


    <!-- --------------------------------
-
- header
-
---- --------------------------------
-->
    <div id="header">
        <div class="wrap">
            <div id="intro">
                <h1 align="center" id="logo">Fine-grained Human-centric Tracklet Segmentation with Single Frame Supervision</h1>
                <div align="center">
                    <table width="80%" border="0" align="center" cellpadding="0" cellspacing="0">
                        <tr>
                            <td width="25%" height="30" align='center'>
                                <a target="_blank">Si Liu
                                    <sup>1</sup>
                                </a>
                            </td>
                            <td width="20%" height="30" align='center'>
                                <a target="_blank">Guanghui Ren
                                    <sup>2</sup>
                                </a>
                            </td>
                            <td width="20%" height="30" align='center'>
                                <a target="_blank">Yao Sun
                                    <sup>2</sup>
                                </a>
                            </td>
                            <td width="20%" height="30" align='center'>
                                <a target="_blank">Jinqiao Wang
                                    <sup>3</sup>
                                </a>
                            </td>

                        </tr>
                    </table>
                    <table>
                        <tr>
                            <td width="20%" height="30" align='center'>
                                <a target="_blank">Changhu Wang
                                    <sup>4</sup>
                                </a>
                            </td>
                            <td width="25%" height="30" align='center'>
                                <a target="_blank">Bo Li
                                    <sup>1</sup>
                                </a>
                            </td>
                            <td width="25%" height="30" align='center'>
                                <a target="_blank">Shuicheng Yan
                                    <sup>5</sup>
                                </a>
                            </td>
                        </tr>
                    </table>
                    <table>
                        <tr>
                            <td height="20" colspan="4" align='center'>
                                <sup>1</sup>Beihang University</td>
                            <p align="center">
                            </p>
                        </tr>
                        <tr>
                            <td height="20" colspan="4" align='center'>
                                <sup>2</sup>Institue of Information Engineering, CAS</td>
                            <p align="center">
                            </p>
                        </tr>
                        <tr>
                            <td height="20" colspan="4" align='center'>
                                <sup>3</sup> National Laboratory of Pattern Recognition, Institute of Automation, CAS</td>
                            <p align="center">
                            </p>
                        </tr>
                        <tr>
                            <td height="20" colspan="4" align='center'>
                                <sup>4</sup>ByteDance AI Lab</td>
                            <p align="center">
                            </p>
                        </tr>
                        <tr>
                            <td height="20" colspan="4" align='center'>
                                <sup>5</sup>Qihoo 360 AI Institute</td>
                            <p align="center">
                            </p>
                        </tr>
                    </table>
                </div>
            </div>
            <div class="nline1"></div>
        </div>
    </div>



    <!-- --------------------------------
-
- Abstract
-
---- --------------------------------
-->
    <div id="cont">
        <div class="wrap">
            <h2 id="subject">Abstract</h1>
                <p align="justify" style="text-indent:2em">
                    In this paper, we target at the Fine-grAined human-Centric Tracklet Segmentation (FACTS) problem, where 12 human parts, e.g.,
                    face, pants, left-leg, are segmented. To reduce the heavy and tedious labeling efforts, FACTS requires
                    only one labeled frame per video during training. The small size of human parts and the labeling scarcity
                    makes FACTS very challenging. Considering adjacent frames of videos are continuous and human usually
                    do not change clothes in a short time, we explicitly consider the pixel-level and frame-level context
                    in the proposed Temporal context Segmentation Network (TSN). On one hand, optical flow is on-line calculated
                    to propagate the pixel-level segmentation results to neighboring frames. On the other hand, frame-level
                    classification likelihood vectors are also propagated to nearby frames. By fully exploiting the pixel-level
                    and framelevel context, TSN indirectly uses the large amount of unlabeled frames during training and
                    produces smooth segmentation results during inference. Experimental results on four video datasets show
                    the superiority of TSN over the state-of-the-arts. The newly annotated datasets can be downloaded via
                    http://liusi-group.com/projects/FACTS for the further studies.
                </p>
                <div class="line"></div>
        </div>
    </div>

    <!-- --------------------------------
-
- Dataset
-
---- --------------------------------
-->
    <div id="cont">
        <div class="wrap">
            <h2 id="subject">Dataset</h1>
                <p align="justify" style="text-indent:2em">
                </p>
                You can download the Indoor, Outdoor, iLIDS-Parsing and Daily Dataset from
                <a href="https://pan.baidu.com/s/1iPcWD2I-ZPvAg8qGM73bPw">Baidu Drive</a>(pwd: atva) or
                <a href="https://drive.google.com/drive/folders/18J90rasWznwPwTHZEckOUaVUz5ANEEFy?usp=sharing">Google Drive</a>
                <div class="line"></div>
        </div>
    </div>

    <!-- --------------------------------
-
- Framework
-
---- --------------------------------
-->
    <div id="cont">
        <div class="wrap">
            <h2 id="subject">Framework</h2>
            <p align="justify" style="text-indent:2em">
            </p>
            <img src="../images/projects/FACTS/framework.png" width="1000px" /> The architecture of the proposed network. The input are {It−l , It−s, It} and the output are the segmentation
            result of It. The triplet is sequentially fed into a feature extraction module and three parallel modules. The
            optical flow and frame parsing modules mine the pixel-level context and refine the parsing results pixel-wisely.
            The frame classification modules generate reliable likelihood vector by regularizing consequent frames to share
            similar global labels. The pixel-level confidence map and frame-level likelihood vector are fused to produce
            the final output.
            <div class="line"></div>
        </div>
    </div>
    <!-- --------------------------------
-
- Experiment
-
---- --------------------------------
-->
    <div id="cont">
        <div class="wrap">
            <h2 id="subject">Experiment</h2>
            <p align="justify" style="text-indent:2em">
            </p>
            <img src="../images/projects/FACTS/experiment.png" width="1000px" /> “l” or “s” means using long or short range nearby frames for pixel-level context propagation, while “c” means
            the confidence is used in the fusion of warped optical flows. The techniques corresponding to “l”, “s”, and “c”
            have been discussed in
            <a href="https://arxiv.org/pdf/1611.09587.pdf">our CVPR paper</a>. “f” refers the method of using frame-level context, and “uf” means using the unsupervised
            fine-tuned optical flow.
            <div class="line"></div>
        </div>
    </div>

    <!-- --------------------------------
-
- Demo
-
---- --------------------------------
-->
    <div id="cont">
        <div class="wrap">
            <h2 id="subject">Demo</h2>
            <p align="justify" style="text-indent:2em">
            </p>
            <video width="1000" autoplay controls="controls">
                <source src="../videos/FACTS/facts.mp4" type="video/mp4" />
                <source src="../videos/FACTS/facts.ogg" type="video/ogg" />
                <source src="../videos/FACTS/facts.webm" type="video/webm" /> Your browser does not support the video tag.
            </video>
            <br>
            <br/>
            <b>Note: the area of neck is labeled as background in groudtruth and only one target human is segmented.
            </b>
            <div class="line"></div>
        </div>
    </div>




    <!-- --------------------------------
-
- References
-
---- --------------------------------
-->
    <div id="cont">
        <div class="wrap">
            <h2 id="subject">References</h2>
            <p align="justify" style="text-indent:2em">
                <ul class="ref-list" align="justify">
                    <li>[1] Si Liu, Changhu Wang, Ruihe Qian, Han Yu, Renda Bao, Yao Sun, Surveillance Video Parsing with Single
                        Frame Supervision, CVPR 2017</li>
                    <li>[2] Han Yu, Guanghui Ren, Ruihe Qian, Yao Sun, Changhu Wang, Hanqing Lu and Si Liu, RSVP: A Real-Time
                        Surveillance Video Parsing System with Single Frame Supervision, ACM MM 2017</li>
                </ul>
        </div>
        <br> </br>
    </div>
    <!-- <div id="footer"></div> -->

    </div>
</body>

</html>